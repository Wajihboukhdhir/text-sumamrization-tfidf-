{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Wajih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string # from this module we will use string.punctuation: A string of all punctuation characters\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our text example\n",
    "text = \"\"\"During my studies, I was able to acquire a set of faculties that enabled me to reinforce my skills in multiple areas: a significant knowledge of the fundamentals of Python in order to implement data science techniques and to build machine learning as well as deep learning models. In addition, my study experience was also reinforced by a series of online courses that delved into web scraping, data analytics and projects in machine learning and time series Analysis. Added to that, I am extremely familiar with SQL to manage data and also data visualization tools like Power BI in order to come up with insights and well-structured decisions. Moreover, my experience with ‘EY Tunisia’ was a golden ticket to develop dashboards and reports that can help analyze business performance and processes using Python and Power BI. In fact, I implemented different techniques to transform data into insights that drive business value. It was an enriching opportunity to delve into the professional world in an efficient way Added to that, my last job in Tunisia as business consultant is an enriching opportunity to delve into the professional world in an efficient way. Besides, starting a professional experience as an international freelancer in the field of data analytics was an asset for me to connect with employers from all over the world. Thus, joining your company would be an experience in perfect match with my ambitions, especially since I am very serious about enhancing my work capabilities besides the fact that I am very motivated and ready to invest my time to integrate successfully the team and to be a driving force in your approach to work.\n",
    "Please find the attached Curriculum Vitae.\n",
    "Thank you in advance for your time and I am at your disposal for any further information and interview. Hoping that my application will hold your attention and in anticipation of a reply, which I hope is favorable, please accept my sincere greetings.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the sent_tokenize function from the Natural Language Toolkit (nltk) is used to split the input text into individual sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['During my studies, I was able to acquire a set of faculties that enabled me to reinforce my skills in multiple areas: a significant knowledge of the fundamentals of Python in order to implement data science techniques and to build machine learning as well as deep learning models.', 'In addition, my study experience was also reinforced by a series of online courses that delved into web scraping, data analytics and projects in machine learning and time series Analysis.', 'Added to that, I am extremely familiar with SQL to manage data and also data visualization tools like Power BI in order to come up with insights and well-structured decisions.', 'Moreover, my experience with ‘EY Tunisia’ was a golden ticket to develop dashboards and reports that can help analyze business performance and processes using Python and Power BI.', 'In fact, I implemented different techniques to transform data into insights that drive business value.', 'It was an enriching opportunity to delve into the professional world in an efficient way Added to that, my last job in Tunisia as business consultant is an enriching opportunity to delve into the professional world in an efficient way.', 'Besides, starting a professional experience as an international freelancer in the field of data analytics was an asset for me to connect with employers from all over the world.', 'Thus, joining your company would be an experience in perfect match with my ambitions, especially since I am very serious about enhancing my work capabilities besides the fact that I am very motivated and ready to invest my time to integrate successfully the team and to be a driving force in your approach to work.', 'Please find the attached Curriculum Vitae.', 'Thank you in advance for your time and I am at your disposal for any further information and interview.', 'Hoping that my application will hold your attention and in anticipation of a reply, which I hope is favorable, please accept my sincere greetings.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# to see how many sentences we have\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocess function converts the input text to lowercase and removes all punctuation using str.maketrans. The list comprehension then applies this function to each sentence in the sentences list, resulting in a list of processed sentences with consistent casing and no punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['during my studies i was able to acquire a set of faculties that enabled me to reinforce my skills in multiple areas a significant knowledge of the fundamentals of python in order to implement data science techniques and to build machine learning as well as deep learning models',\n",
       " 'in addition my study experience was also reinforced by a series of online courses that delved into web scraping data analytics and projects in machine learning and time series analysis',\n",
       " 'added to that i am extremely familiar with sql to manage data and also data visualization tools like power bi in order to come up with insights and wellstructured decisions',\n",
       " 'moreover my experience with ‘ey tunisia’ was a golden ticket to develop dashboards and reports that can help analyze business performance and processes using python and power bi',\n",
       " 'in fact i implemented different techniques to transform data into insights that drive business value',\n",
       " 'it was an enriching opportunity to delve into the professional world in an efficient way added to that my last job in tunisia as business consultant is an enriching opportunity to delve into the professional world in an efficient way',\n",
       " 'besides starting a professional experience as an international freelancer in the field of data analytics was an asset for me to connect with employers from all over the world',\n",
       " 'thus joining your company would be an experience in perfect match with my ambitions especially since i am very serious about enhancing my work capabilities besides the fact that i am very motivated and ready to invest my time to integrate successfully the team and to be a driving force in your approach to work',\n",
       " 'please find the attached curriculum vitae',\n",
       " 'thank you in advance for your time and i am at your disposal for any further information and interview',\n",
       " 'hoping that my application will hold your attention and in anticipation of a reply which i hope is favorable please accept my sincere greetings']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Clean and prepare the text (remove punctuation and lowercase)\n",
    "#str.maketrans('', '', string.punctuation) creates a translation table that removes all punctuation characters. \n",
    "# When this translation table is applied to a string using text.translate(trans_table), it removes any characters from the string that are in string.punctuation.\n",
    "def preprocess(text):\n",
    "    return text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "processed_sentences = [preprocess(sentence) for sentence in sentences]\n",
    "\n",
    "processed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TfidfVectorizer is initialized with English stop words removed, which means common words like \"the\" or \"is\" will not be considered in the analysis. The fit_transform method then computes the TF-IDF (Term Frequency-Inverse Document Frequency) matrix, representing each sentence in the processed_sentences list as a vector in the feature space defined by the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 103)\t0.2032252323934614\n",
      "  (0, 0)\t0.2032252323934614\n",
      "  (0, 2)\t0.2032252323934614\n",
      "  (0, 98)\t0.2032252323934614\n",
      "  (0, 48)\t0.2032252323934614\n",
      "  (0, 40)\t0.2032252323934614\n",
      "  (0, 91)\t0.2032252323934614\n",
      "  (0, 100)\t0.2032252323934614\n",
      "  (0, 79)\t0.2032252323934614\n",
      "  (0, 13)\t0.2032252323934614\n",
      "  (0, 99)\t0.2032252323934614\n",
      "  (0, 71)\t0.2032252323934614\n",
      "  (0, 54)\t0.2032252323934614\n",
      "  (0, 89)\t0.1737095302955632\n",
      "  (0, 82)\t0.1737095302955632\n",
      "  (0, 61)\t0.2032252323934614\n",
      "  (0, 28)\t0.12325210428704043\n",
      "  (0, 95)\t0.2032252323934614\n",
      "  (0, 107)\t0.1737095302955632\n",
      "  (0, 18)\t0.2032252323934614\n",
      "  (0, 74)\t0.1737095302955632\n",
      "  (0, 72)\t0.3474190605911264\n",
      "  (0, 30)\t0.2032252323934614\n",
      "  (0, 77)\t0.2032252323934614\n",
      "  (1, 28)\t0.14466250680764378\n",
      "  :\t:\n",
      "  (7, 65)\t0.20967080559478515\n",
      "  (7, 105)\t0.20967080559478515\n",
      "  (7, 106)\t0.20967080559478515\n",
      "  (7, 37)\t0.20967080559478515\n",
      "  (7, 52)\t0.20967080559478515\n",
      "  (7, 12)\t0.20967080559478515\n",
      "  (8, 15)\t0.5773502691896258\n",
      "  (8, 26)\t0.5773502691896258\n",
      "  (8, 117)\t0.5773502691896258\n",
      "  (9, 110)\t0.31865342031702687\n",
      "  (9, 108)\t0.42390093128471895\n",
      "  (9, 5)\t0.42390093128471895\n",
      "  (9, 35)\t0.42390093128471895\n",
      "  (9, 63)\t0.42390093128471895\n",
      "  (9, 67)\t0.42390093128471895\n",
      "  (10, 60)\t0.31622776601683794\n",
      "  (10, 11)\t0.31622776601683794\n",
      "  (10, 58)\t0.31622776601683794\n",
      "  (10, 16)\t0.31622776601683794\n",
      "  (10, 10)\t0.31622776601683794\n",
      "  (10, 93)\t0.31622776601683794\n",
      "  (10, 59)\t0.31622776601683794\n",
      "  (10, 50)\t0.31622776601683794\n",
      "  (10, 1)\t0.31622776601683794\n",
      "  (10, 56)\t0.31622776601683794\n"
     ]
    }
   ],
   "source": [
    "#Vectorize the sentences using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Just to better understand tfidf matrix\n",
    "\n",
    "The todense() method converts the sparse tfidf_matrix into a dense matrix, allowing for easier visualization of the TF-IDF values as a full array. The get_feature_names_out() method retrieves the list of unique terms (words) corresponding to the columns of the matrix, which represent the vocabulary used in the TF-IDF transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20322523 0.         0.20322523 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.25713503 0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.31622777 0.         ... 0.         0.         0.        ]]\n",
      "['able' 'accept' 'acquire' 'added' 'addition' 'advance' 'ambitions'\n",
      " 'analysis' 'analytics' 'analyze' 'anticipation' 'application' 'approach'\n",
      " 'areas' 'asset' 'attached' 'attention' 'bi' 'build' 'business'\n",
      " 'capabilities' 'come' 'company' 'connect' 'consultant' 'courses'\n",
      " 'curriculum' 'dashboards' 'data' 'decisions' 'deep' 'delve' 'delved'\n",
      " 'develop' 'different' 'disposal' 'drive' 'driving' 'efficient'\n",
      " 'employers' 'enabled' 'enhancing' 'enriching' 'especially' 'experience'\n",
      " 'extremely' 'ey' 'fact' 'faculties' 'familiar' 'favorable' 'field'\n",
      " 'force' 'freelancer' 'fundamentals' 'golden' 'greetings' 'help' 'hold'\n",
      " 'hope' 'hoping' 'implement' 'implemented' 'information' 'insights'\n",
      " 'integrate' 'international' 'interview' 'invest' 'job' 'joining'\n",
      " 'knowledge' 'learning' 'like' 'machine' 'manage' 'match' 'models'\n",
      " 'motivated' 'multiple' 'online' 'opportunity' 'order' 'perfect'\n",
      " 'performance' 'power' 'processes' 'professional' 'projects' 'python'\n",
      " 'ready' 'reinforce' 'reinforced' 'reply' 'reports' 'science' 'scraping'\n",
      " 'series' 'set' 'significant' 'skills' 'sql' 'starting' 'studies' 'study'\n",
      " 'successfully' 'team' 'techniques' 'thank' 'ticket' 'time' 'tools'\n",
      " 'transform' 'tunisia' 'using' 'value' 'visualization' 'vitae' 'way' 'web'\n",
      " 'wellstructured' 'work' 'world']\n"
     ]
    }
   ],
   "source": [
    "# Convert the matrix to a dense format and print it\n",
    "dense_matrix = tfidf_matrix.todense()\n",
    "print(dense_matrix)\n",
    "\n",
    "# Get the terms (words) corresponding to the columns\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- back to our porject:\n",
    "\n",
    "The cosine_similarity function computes the pairwise cosine similarity between all the sentences in the tfidf_matrix. This results in a similarity matrix where each value represents how similar two sentences are, based on the angle between their TF-IDF vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cosine Similarity between sentences\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum(axis=1) method calculates the sum of cosine similarity scores for each sentence, effectively aggregating the similarity of each sentence with all other sentences. The resulting sentence_scores array provides a measure of the overall similarity of each sentence to the others in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.34105807 1.4526298  1.44538314 1.33749316 1.42345819 1.30098255\n",
      " 1.47683434 1.20918678 1.         1.10736023 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Rank sentences based on similarity (here using the sum of similarities for each sentence)\n",
    "sentence_scores = cosine_sim.sum(axis=1)\n",
    "print(sentence_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line selects the top 3 most similar sentences by sorting the sentence_scores array in descending order and using the indices to retrieve the corresponding sentences from the sentences list. The argsort() function is used to obtain the indices of the sentences with the highest similarity scores, and the [::-1] reverses the order to get the highest scores first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top N sentences for the summary (since my text contains 11 senteces, we will use 3 sentences to summarize)\n",
    "top_n_sentences = [sentences[i] for i in sentence_scores.argsort()[-3:][::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line joins the top 3 most similar sentences (stored in top_n_sentences) into a single string, with each sentence separated by a space. The result is a concise summary that includes the most relevant or representative sentences based on their cosine similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Besides, starting a professional experience as an international freelancer in the field of data analytics was an asset for me to connect with employers from all over the world. In addition, my study experience was also reinforced by a series of online courses that delved into web scraping, data analytics and projects in machine learning and time series Analysis. Added to that, I am extremely familiar with SQL to manage data and also data visualization tools like Power BI in order to come up with insights and well-structured decisions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 7: Print the summary\n",
    "summary = \" \".join(top_n_sentences)\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
